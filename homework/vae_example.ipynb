{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE on MINIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we play with VAE model on the MNIST dataset. The materials are from https://github.com/pytorch/examples/blob/master/vae/main.py.\n",
    "\n",
    "- We use python 3.0\n",
    "- With more explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the modules\n",
    "\n",
    "## load pytorch module\n",
    "import torch\n",
    "\n",
    "## load torch data manager: Dataset, DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "## load neural network module\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "## load optimizer\n",
    "from torch import optim\n",
    "\n",
    "## load modules for MNIST dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "## load other python modules\n",
    "### plt to view the image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "## download MNIST trainign dataset \n",
    "train_dataset: Dataset = datasets.MNIST(\"./data\", train = True, download = True,\n",
    "                                       transform = transforms.ToTensor())\n",
    "## class Dataset provides:\n",
    "## - function \"__len__\": to get the size of the dataset\n",
    "## - function \"__getitem__\": to get the data point by the index\n",
    "\n",
    "## show the data size\n",
    "print(train_dataset)\n",
    "## show one data point\n",
    "## each data point has two element: one is the image, one is which number it is.\n",
    "\n",
    "## print the tensor size: (1, 28, 28): \n",
    "## the first dim: 1 is the number of channel (only one) here\n",
    "## the second and third dim: the width and height for this channel.\n",
    "print(train_dataset[0][0].shape)\n",
    "\n",
    "## print the number\n",
    "print(train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8749393e50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## let's view this image\n",
    "plt.imshow(train_dataset[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Dataloader to wrap up the dataset\n",
    "\n",
    "During the learning process, we will use stochasitic optmization, which means we will shuffle the sample, and put a small part of the dataset into the optimizer. You can do this by yourself, but pytorch actually provides the data manager named Dataloader to simplify this process, and it is usually used together with the Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## you can choose whatever batch_size you want.\n",
    "## shuffle = True, means every epoch, we will shuffle the samplers \n",
    "## by the sampling algorithm you or the default one.\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle = True)\n",
    "## we don't need to shuffle the samples during test.\n",
    "test_loader = DataLoader(test_dataset, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (fc1): Linear(in_features=784, out_features=400, bias=True)\n",
      "  (fc21): Linear(in_features=400, out_features=20, bias=True)\n",
      "  (fc22): Linear(in_features=400, out_features=20, bias=True)\n",
      "  (fc3): Linear(in_features=20, out_features=400, bias=True)\n",
      "  (fc4): Linear(in_features=400, out_features=784, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## All the neural networks are implemented as the class of nn.Module in pytorch.\n",
    "## It need to provide the function named \"forward\" to declare how it transform a tensor.\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    ## __init__ is the basic syntax in python. when you want to describe a class, you need this \n",
    "    ## to tell python how to initialize this class.\n",
    "    def __init__(self):\n",
    "        ## all the modules need this firstly to initialize itself.\n",
    "        super().__init__()\n",
    "        ## then we define several neural network structure\n",
    "        \n",
    "        ## pytorch provides lots of neural network structures under the nn module. you can check it yourself.\n",
    "        ## here we just use the simpliest linear transformation\n",
    "        \n",
    "        ## [QUESTION]: why we use the numebr 784 here?\n",
    "        \n",
    "        ## this linear transformation is a typical one-layer fully connected neural network\n",
    "        ## with input dim 784, and output dim 400.\n",
    "        \n",
    "        ## [QUESTION]: how many parameters we use for the five fully connected neural networks? \n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "        \n",
    "    ## now let's move forward to implement the VAE, a typical VAE involves\n",
    "    ## - an encoder: define q(z|x), i.e., the mean and variance of the Gaussian distribution\n",
    "    ## - a decoder: defile p(x|z)\n",
    "    ## - the implementation of reparameterization\n",
    "    \n",
    "    def encode(self, x):\n",
    "        ## generate the mean and log of the variance of q(z|x)\n",
    "        ## we assume that z_i and z_j are independent for any z_i, z_j \\in z and i != j\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        \n",
    "        ## [QUESTION]: what's dimention or size of the two outputs? \n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):    \n",
    "        std = torch.exp(0.5*logvar)\n",
    "        \n",
    "        ## we sample the eps from a standard normal distribution, \n",
    "        ## whose shape or dimention is the same as std\n",
    "        \n",
    "        ## [QUESTION]: do you know how many samples we use to approxmate the derivates of ELBO\n",
    "        ## base on the expression below? \n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        ## [QUESTION]: why we need to do the reparameterization?\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        ## [QUESTION]: do you know what's the p(x/z) is used here?\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## x is a typical mini-batch samples.\n",
    "        ## view function is used to re-define the dimension of x in order to output the  \n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar  \n",
    "    \n",
    "model = VAE()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's define the loss function of VAE. \n",
    "## There are two parts:\n",
    "## - the loss of reconstructio, i.e., - p(x|z)\n",
    "## - the KL divergence towards the prior, i.e., KL(q(z|x) ||  p(z))\n",
    "\n",
    "## binary cross entropy loss\n",
    "reconst_loss = nn.BCELoss(reduction = 'sum')\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    rcloss = reconst_loss(recon_x, x.view(-1, 784))\n",
    "    ## analytic formulation of KL divergence of Gaussian distribution. \n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return rcloss + kld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lots of the optimizers you can select in torch.\n",
    "## nowadays, people use Adam a lot.\n",
    "## An interesting thing: the first author of Adam is the same first author of VAE.\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start to train VAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Epoch: once we run through all the samples in the training dataset, we finish one epoch. \n",
    "def train(epoch):\n",
    "    ## [IMPORTANT]: in pytorch, model will update the parameters when it's in training state.\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    ## we ignore the labels of MNIST, and only use the images.\n",
    "    for batch_id, (x, _) in enumerate(train_loader):\n",
    "        ## [IMPORTANT]: each differentiable tensor in pytorch will recognize the previous gradient.\n",
    "        ## so we have to set them as zeros before we compute next time.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, logvar = model(x)\n",
    "        loss = loss_function(recon_batch, x, mu, logvar)\n",
    "        \n",
    "        ## we do the back propagation on the tensor generated by the loss function.\n",
    "        loss.backward()\n",
    "        \n",
    "        ## then update the parameters by the optmizer.\n",
    "        optimizer.step()\n",
    "        \n",
    "        ## let's remember the loss\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        ## print the loss\n",
    "        if batch_id % 500 == 0:\n",
    "            print(f\"Train Epoch {epoch} [{batch_id}]: loss {loss.item() / len(x)}\")\n",
    "    print(f\"Epoch: {epoch} average loss: {train_loss / len(train_loader.dataset)}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 [0]: loss 548.3065185546875\n",
      "Train Epoch 0 [500]: loss 136.61532592773438\n",
      "Train Epoch 0 [1000]: loss 129.72923278808594\n",
      "Train Epoch 0 [1500]: loss 116.49365234375\n",
      "Epoch: 0 average loss: 136.2928796142578\n",
      "Train Epoch 1 [0]: loss 112.4651107788086\n",
      "Train Epoch 1 [500]: loss 123.67293548583984\n",
      "Train Epoch 1 [1000]: loss 108.67465209960938\n",
      "Train Epoch 1 [1500]: loss 113.25799560546875\n",
      "Epoch: 1 average loss: 113.12685036621093\n",
      "Train Epoch 2 [0]: loss 113.07672119140625\n",
      "Train Epoch 2 [500]: loss 110.04753112792969\n",
      "Train Epoch 2 [1000]: loss 110.82366943359375\n",
      "Train Epoch 2 [1500]: loss 116.17874145507812\n",
      "Epoch: 2 average loss: 109.94258452555339\n",
      "Train Epoch 3 [0]: loss 115.4400863647461\n",
      "Train Epoch 3 [500]: loss 115.77740478515625\n",
      "Train Epoch 3 [1000]: loss 109.57640838623047\n",
      "Train Epoch 3 [1500]: loss 108.12872314453125\n",
      "Epoch: 3 average loss: 108.5396646118164\n",
      "Train Epoch 4 [0]: loss 108.8280258178711\n",
      "Train Epoch 4 [500]: loss 113.8145523071289\n",
      "Train Epoch 4 [1000]: loss 108.45506286621094\n",
      "Train Epoch 4 [1500]: loss 106.1849365234375\n",
      "Epoch: 4 average loss: 107.66387190755208\n",
      "Train Epoch 5 [0]: loss 93.9540786743164\n",
      "Train Epoch 5 [500]: loss 97.2364501953125\n",
      "Train Epoch 5 [1000]: loss 107.56150817871094\n",
      "Train Epoch 5 [1500]: loss 103.14619445800781\n",
      "Epoch: 5 average loss: 107.02230043538411\n",
      "Train Epoch 6 [0]: loss 113.92561340332031\n",
      "Train Epoch 6 [500]: loss 110.44760131835938\n",
      "Train Epoch 6 [1000]: loss 105.19571685791016\n",
      "Train Epoch 6 [1500]: loss 109.75306701660156\n",
      "Epoch: 6 average loss: 106.52937862548828\n",
      "Train Epoch 7 [0]: loss 111.72636413574219\n",
      "Train Epoch 7 [500]: loss 107.7178955078125\n",
      "Train Epoch 7 [1000]: loss 110.54678344726562\n",
      "Train Epoch 7 [1500]: loss 106.5859375\n",
      "Epoch: 7 average loss: 106.124308207194\n",
      "Train Epoch 8 [0]: loss 102.07904052734375\n",
      "Train Epoch 8 [500]: loss 106.7297592163086\n",
      "Train Epoch 8 [1000]: loss 119.59136962890625\n",
      "Train Epoch 8 [1500]: loss 117.94959259033203\n",
      "Epoch: 8 average loss: 105.79906294352213\n",
      "Train Epoch 9 [0]: loss 103.80705261230469\n",
      "Train Epoch 9 [500]: loss 103.38383483886719\n",
      "Train Epoch 9 [1000]: loss 109.30065155029297\n",
      "Train Epoch 9 [1500]: loss 101.52336883544922\n",
      "Epoch: 9 average loss: 105.55134681396484\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train(epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's view the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## no grad means the computations below will not be used for optimization.\n",
    "%matplotlib inline\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        sample = torch.randn(1, 20)\n",
    "        rdm_image = model.decode(sample).view(28, 28)\n",
    "        plt.imshow(rdm_image)\n",
    "\n",
    "## [QUESION*]: can you show the generated images given a specific number?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
